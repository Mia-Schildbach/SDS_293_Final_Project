{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e140c5-1f47-4bd2-8f24-7161b0a2b6dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.13.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838806ef-696c-4a1d-8724-7251a8da5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data dimensions\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c6584-3862-4579-a336-3fc170390cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa48c08-8c58-4296-b279-461baa5b397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"MISSING VALUES:\")\n",
    "train_missing = train.isnull().sum().sum()\n",
    "test_missing = test.isnull().sum().sum()\n",
    "print(f\"Train total missing: {train_missing}\")\n",
    "print(f\"Test total missing: {test_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3beb9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aa319-a44e-4e9c-8c6e-95bf9e25a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_missing > 0:\n",
    "    print(\"Train columns with missing values:\")\n",
    "    missing_cols = train.isnull().sum()\n",
    "    print(missing_cols[missing_cols > 0])\n",
    "\n",
    "if test_missing > 0:\n",
    "    print(\"\\nTest columns with missing values:\")\n",
    "    missing_cols = test.isnull().sum()\n",
    "    print(missing_cols[missing_cols > 0])\n",
    "\n",
    "# Filling in the missing values with median if any exists\n",
    "if train_missing > 0 or test_missing > 0:\n",
    "    print(\" Filling missing values with median...\")\n",
    "    train.fillna(train.median(), inplace=True)\n",
    "    test.fillna(test.median(), inplace=True)\n",
    "    print(\" Missing values filled\")\n",
    "else:\n",
    "    print(\" No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d52a5-8815-472d-b97f-c8116bca3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data more\n",
    "\n",
    "print(\"\\nTARGET DISTRIBUTION:\")\n",
    "print(train['target'].value_counts())\n",
    "print(f\"Transaction rate: {train['target'].mean():.2%}\") # percentage that made a transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f57fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataset of just sales \n",
    "\n",
    "train_sale_subset = train[train[\"target\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d32d1-c0ea-4f82-9d5a-752a68b849c8",
   "metadata": {},
   "source": [
    "10% of people made a transaction. So we have to use AUC as a metric instead of accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5007e-57e1-4271-a331-18f5ace19864",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train['ID_code']\n",
    "test_ids = test['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056af00a-3b0a-4330-aaf7-0f36d7c4e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and target\n",
    "X = train.drop(['ID_code', 'target'], axis=1)\n",
    "y = train['target']\n",
    "X_test = test.drop('ID_code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9334f2-6a6e-4603-a7c8-98541f7e5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "X['mean'] = X.mean(axis=1)\n",
    "X['std'] = X.std(axis=1)\n",
    "X['max'] = X.max(axis=1)\n",
    "X['min'] = X.min(axis=1)\n",
    "\n",
    "\n",
    "print(f\"Original features: 200\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Added: mean, std, max, min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef652f-1a0f-4e65-8833-fb2aa1c87fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correlation with target\n",
    "correlations = pd.DataFrame(X).corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"Top 20 features by correlation with target:\")\n",
    "print(correlations.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46f2af-e9ba-48dd-9413-c1eb24037216",
   "metadata": {},
   "source": [
    "The features above have the most correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf5cfb-927b-449d-8bad-14a8c3d9adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Target distribution\n",
    "axes[0, 0].bar(['No Transaction', 'Transaction'], y.value_counts().values)\n",
    "axes[0, 0].set_title('Target Distribution')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Top correlated feature\n",
    "top_feature = correlations.index[0]\n",
    "axes[0, 1].hist([X[y==0][top_feature], X[y==1][top_feature]], \n",
    "                label=['No Transaction', 'Transaction'], bins=30)\n",
    "axes[0, 1].set_title(f'{top_feature} by Target')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "\n",
    "# Mean feature by target\n",
    "axes[1, 0].hist([X[y==0]['mean'], X[y==1]['mean']], \n",
    "                label=['No Transaction', 'Transaction'], bins=30)\n",
    "axes[1, 0].set_title('Mean Feature by Target')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "\n",
    "# Stabdard deviation feature by target\n",
    "axes[1, 1].hist([X[y==0]['std'], X[y==1]['std']], \n",
    "                label=['No Transaction', 'Transaction'], bins=30)\n",
    "axes[1, 1].set_title('Std Feature by Target')\n",
    "axes[1, 1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f443d1f-1cf9-4483-b656-6e38e76c8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features correlation heatmap\n",
    "top_10 = correlations.head(10).index\n",
    "corr_matrix = X[top_10].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap - Top 10 Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311be9c-fb48-4b60-8d07-1f792a02c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with different scaling and PCA options\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "\n",
    "# Using subset of data for faster results\n",
    "subset_size = 10000  # Use only 10k samples for grid search\n",
    "X_train_subset, _, y_train_subset, _ = train_test_split(\n",
    "    X_train, y_train, train_size=subset_size, random_state=42, stratify=y_train\n",
    ")\n",
    "print(f\"\\nUsing subset of {subset_size} samples for grid search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136d99b-5804-4933-a4de-da2ae7edc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#might want to rename this later?\n",
    "\n",
    "preprocessing_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Will be replaced in grid search\n",
    "    ('pca', PCA()),  # Will be replaced in grid search\n",
    "    ('model', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'pca__n_components': [None, 5, 10, 25, 50, 100],\n",
    "    'model__C': [0.1, 1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1878c-cc03-40d8-bee5-268aef3fa025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on subset\n",
    "grid_search = GridSearchCV(\n",
    "    preprocessing_pipe, \n",
    "    param_grid, \n",
    "    cv=3,  \n",
    "    scoring='roc_auc',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f0885-d237-4a94-a731-0b9cace04cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "print(f\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest CV AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set full\n",
    "val_auc = roc_auc_score(y_val, grid_search.predict_proba(X_val)[:, 1])\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Save best preprocessing pipeline\n",
    "best_preprocessing = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa49dd7-7dc6-40cd-a4b8-ff5f9d138161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying a Decision Tree Classifier \n",
    "#Based off code from my (Mia's) lab 11, and Aarya's code above \n",
    "\n",
    "tree_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('pca', PCA()),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "tree_parms = {\n",
    "    'model__max_depth': [2,3,4],\n",
    "    'model__min_samples_split': [10,20,40],\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
    "    'pca__n_components': [None, 5, 10, 25, 50, 100]\n",
    "}\n",
    "\n",
    "tree_grid_res = GridSearchCV(\n",
    "    estimator = tree_pipe, \n",
    "    param_grid = tree_parms, \n",
    "    cv=5, \n",
    "    scoring = 'roc_auc').fit(X_train_subset, y_train_subset)\n",
    "\n",
    "print(tree_grid_res.best_estimator_)\n",
    "print(tree_grid_res.best_score_)\n",
    "\n",
    "sale_tree = tree_grid_res.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe_2 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('model', DecisionTreeClassifier(max_depth=4, min_samples_split=10))\n",
    "]).fit(X_train_subset, y_train_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082a57b",
   "metadata": {},
   "source": [
    "sale_tree = DecisionTreeClassifier(max_depth=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e59d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree diagram\n",
    "#Code adapted from lab 7 \n",
    "#Used google's AI overview to debug this code\n",
    "\n",
    "tree_model = tree_pipe_2.named_steps['model']\n",
    "\n",
    "from sklearn.tree import plot_tree, _tree\n",
    "\n",
    "plot_tree(\n",
    "    tree_model,\n",
    "    # We need to convert the predictor variable names to a list:\n",
    "    feature_names = X_train_subset.columns.tolist(), \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print tree model\n",
    "\n",
    "print(tree_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55468b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gonna try re-doing this with less splits \n",
    "tree_pipe_3 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('model', DecisionTreeClassifier(max_depth=3, min_samples_split=10))\n",
    "]).fit(X_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree diagram\n",
    "#Code adapted from lab 7 \n",
    "#Used google's AI overview to debug this code\n",
    "\n",
    "tree_model = tree_pipe_3.named_steps['model']\n",
    "\n",
    "plot_tree(\n",
    "    tree_model,\n",
    "    # We need to convert the predictor variable names to a list:\n",
    "    feature_names = X_train_subset.columns.tolist(), \n",
    ")\n",
    "plt.show()\n",
    "\n",
    "#seems like variables 2 and 3 are important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b86d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe_4 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('model', DecisionTreeClassifier(max_depth=2, min_samples_split=10))\n",
    "]).fit(X_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c07365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree diagram\n",
    "#Code adapted from lab 7 \n",
    "#Used google's AI overview to debug this code\n",
    "\n",
    "tree_model = tree_pipe_4.named_steps['model']\n",
    "\n",
    "plot_tree(\n",
    "    tree_model,\n",
    "    # We need to convert the predictor variable names to a list:\n",
    "    feature_names = X_train_subset.columns.tolist(), \n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5d4b1",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe422629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC())\n",
    "])\n",
    "\n",
    "svc_parms = {\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__C' :  [0.1, 1, 10,], \n",
    "    'model__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "svc_grid_res = GridSearchCV(\n",
    "    estimator = svc_pipe, \n",
    "    param_grid = svc_parms, \n",
    "    cv=3,\n",
    "    scoring = 'roc_auc').fit(X_train_subset, y_train_subset)\n",
    "\n",
    "\n",
    "\n",
    "print(svc_grid_res.best_estimator_)\n",
    "print(svc_grid_res.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
